export const en = {
	nav: {
		introduction: "Introduction",
		installation: "Installation",
		quickStart: "Quick Start",
		usage: "Usage",
		apiReference: "API Reference",
		useLiveness: "useLiveness",
		examples: "Examples",
		configuration: "Configuration",
		faq: "FAQ",
	},
	intro: {
		title: "react-livecheck",
		description:
			"React hook for person liveness detection — verify that a user is a real live person via blink detection (MediaPipe Face Mesh). Build your own UI; the hook handles camera, face detection, and pass/fail state.",
		whatIs: "What is liveness detection?",
		whatIsBody:
			"Liveness detection ensures that the user in front of the camera is a real, live person (e.g. for identity verification or anti-spoofing). This library uses blink detection: the user is asked to blink a few times, and the hook reports success when the required number of blinks is detected.",
		mediapipe: "MediaPipe Face Mesh",
		mediapipeBody:
			"Under the hood, react-livecheck uses MediaPipe Face Mesh to detect facial landmarks. Blinks are detected via the Eye Aspect Ratio (EAR) on both eyes. The face must be roughly centered in frame for a blink to count.",
		singleInstance: "One instance per page",
		singleInstanceBody: "Use one useLiveness instance per page (one camera stream). Multiple instances may conflict.",
	},
	installation: {
		title: "Installation",
		installCommand: "pnpm add react-livecheck",
		requirements: "Requirements",
		reqReact: "React 19 and peer dependency react",
		reqHttps: "HTTPS in production (or localhost). Camera access requires a secure context.",
		reqBrowser: "Modern browser with navigator.mediaDevices and MediaPipe support (Chrome, Firefox, Safari, Edge)",
		reqInstance: "One useLiveness instance per page",
	},
	quickStart: {
		title: "Quick Start",
		description: "Minimal example: import the hook, call it, and attach videoRef to a video element.",
	},
	usage: {
		title: "Usage",
		description: "Full example with error handling, face bounding box overlay, and retry.",
	},
	api: {
		title: "API Reference",
		useLivenessOptions: "useLiveness(options?)",
		options: "Options",
		return: "Return",
		errorCodes: "Error codes (LivenessErrorCode)",
		option: "Option",
		type: "Type",
		default: "Default",
		description: "Description",
		property: "Property",
		code: "Code",
		meaning: "Meaning",
	},
	components: {
		title: "Components / Modules",
		useLivenessTitle: "useLiveness hook",
		useLivenessPurpose:
			"The main API. Call useLiveness(options) in your component to get videoRef, blinkCount, passed, error, and other state. Attach videoRef to a <video> element; the hook starts the camera and runs MediaPipe Face Mesh to detect blinks.",
		behavior: "Behavior",
		behaviorBlink:
			"Blinks are detected using Eye Aspect Ratio (EAR). A blink is counted when EAR drops below a threshold then rises again. Only one face should be in frame; the face should be roughly centered.",
		types: "Exported types",
		typesBody:
			"FaceBoundingBox, Landmarks, LivenessError, UseLivenessOptions, UseLivenessReturn, UseLivenessCameraOptions, UseLivenessFaceMeshOptions.",
		apiLink: "API",
		apiLinkSuffix: "for full options, return value, and error codes.",
	},
	examples: {
		title: "Examples",
		basic: "Basic blink check",
		basicDesc: "Minimal setup with requiredBlinks and callbacks.",
		withOverlay: "With face bounding box and retry",
		withOverlayDesc: "Full UI: error handling, face overlay, retry button (from README).",
		withTimeout: "With face detection timeout",
		withTimeoutDesc: "Fails with FACE_NOT_DETECTED if no face is seen within the given time.",
		customConfig: "Custom camera and FaceMesh options",
		customConfigDesc: "Override camera resolution and FaceMesh confidence thresholds.",
	},
	configuration: {
		title: "Configuration",
		intro: "All configuration is passed via the options object to useLiveness.",
		camera: "Camera",
		cameraDesc: "width, height, facingMode. Defaults: 640×480, user.",
		faceMesh: "FaceMesh",
		faceMeshDesc: "maxNumFaces, minDetectionConfidence, minTrackingConfidence. Defaults: 1, 0.7, 0.7.",
		locateFile: "locateFile",
		locateFileDesc: "Function (file: string) => string for MediaPipe model URLs. Default: jsDelivr CDN.",
		faceDetectionTimeout: "faceDetectionTimeout",
		faceDetectionTimeoutDesc: "Timeout in ms before FACE_NOT_DETECTED if no face seen. 0 = disabled.",
	},
	faq: {
		title: "FAQ",
		q1: "Why does the camera not start?",
		a1: "Ensure you use HTTPS (or localhost). getUserMedia is blocked on plain HTTP. If the user denied permission, show a message and use retry() after they grant access.",
		q2: "Can I use multiple useLiveness instances on one page?",
		a2: "No. Use one instance per page (one camera stream). Multiple instances may conflict.",
		q3: "The model fails to load (MODEL_LOAD_FAILED). What can I do?",
		a3: "Some networks block the default CDN. Use the locateFile option to self-host the MediaPipe Face Mesh files or point to an allowed URL.",
		q4: "Video does not play (PLAY_FAILED).",
		a4: "Browsers may block autoplay. Prompt the user to tap a button to start, then call retry() or ensure the video element is played after user gesture.",
		q5: "Which browsers are supported?",
		a5: "Modern browsers with navigator.mediaDevices and support for MediaPipe (Chrome, Firefox, Safari, Edge). Test on your target devices.",
	},
	common: {
		tryAgain: "Try again",
		allowCamera: "Please allow camera access.",
		startingCamera: "Starting camera…",
		positionFace: "Position your face in the frame.",
		blinks: "Blinks",
		passed: "Passed!",
		liveDemo: "Live demo",
		tryDemo: "Try",
		github: "GitHub",
		npm: "npm",
		mitLicense: "MIT License",
	},
	demo: {
		title: "Face liveness check",
		description: "Allow camera access and blink twice to verify you're a real person.",
		close: "Close",
		retry: "Try again",
		successTitle: "Liveness verified",
		successMessage: "You've passed the blink check.",
		autoClose: "Closing in 5 seconds…",
	},
	landing: {
		github: "GitHub",
		npm: "npm",
		mitLicense: "MIT License",
	},
	apiTable: {
		option: "Option",
		type: "Type",
		default: "Default",
		description: "Description",
		property: "Property",
		optRequiredBlinksDesc: "Blinks required to pass.",
		optOnSuccessDesc: "Called when liveness passes.",
		optOnErrorDesc: "Called on error.",
		optLocateFileDesc: "MediaPipe model URLs.",
		optCameraDesc: "Camera constraints.",
		optFaceMeshDesc: "FaceMesh options.",
		optFaceDetectionTimeoutDesc: "Timeout before FACE_NOT_DETECTED.",
		retVideoRef: "Attach to <video>",
		retBlinkCount: "Detected blinks.",
		retPassed: "true when required blinks reached.",
		retError: "Current error.",
		retIsReady: "Camera started.",
		retIsFaceDetected: "Face in frame.",
		retFaceBoundingBox: "Normalized box (0–1) for overlay.",
		retRetry: "Clear error and restart.",
		retryNote: "On MULTIPLE_FACES, FACE_NOT_DETECTED, MODEL_LOAD_FAILED or PLAY_FAILED — call retry() to restart.",
		errAborted: "Camera request was aborted.",
		errCameraInUse: "Camera in use by another app/tab.",
		errCameraNotFound: "No camera device found.",
		errFaceNotDetected: "No face seen within timeout.",
		errModelLoadFailed: "MediaPipe model failed to load.",
		errMultipleFaces: "More than one face; use retry().",
		errNotAllowed: "Camera not supported (e.g. non-HTTPS).",
		errOverconstrained: "Camera constraints not supported.",
		errPermissionDenied: "User denied camera access.",
		errPlayFailed: "Video failed to play (e.g. autoplay).",
		errUnknown: "Other errors.",
	},
} as const;
